{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T12:35:11.619381Z",
     "start_time": "2020-09-20T12:35:10.459510Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class AggrE(object):\n",
    "    def __init__(self,\n",
    "                 sampled_contexts,\n",
    "                 masks,\n",
    "                 hops,\n",
    "                 epoch=20,\n",
    "                 batch_size=128,\n",
    "                 dim=64,\n",
    "                 l2=1e-7,\n",
    "                 lr=5e-3,\n",
    "                 negtive_num=-1):\n",
    "        \n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.output_dim = dim\n",
    "        self.l2 = l2\n",
    "        self.lr = lr\n",
    "        self.negtive_num = negtive_num\n",
    "        self.e_sampled_contexts = sampled_contexts[0]\n",
    "        self.masks = masks[0]\n",
    "        self.r_sampled_contexts = sampled_contexts[1]\n",
    "        self.r_masks = masks[1]\n",
    "        self.hops = hops\n",
    "\n",
    "        self.n_entities = n_entity\n",
    "        self.n_relations = n_relation\n",
    "\n",
    "        self._build_inputs()\n",
    "        self._build_embedding()  \n",
    "        self._build_aggre()\n",
    "        self._build_train()\n",
    "        self._build_eval()\n",
    "\n",
    "    def _build_inputs(self):\n",
    "\n",
    "        self.heads = tf.placeholder(tf.int32, [self.batch_size], name='heads')\n",
    "        self.relations = tf.placeholder(tf.int32, [self.batch_size], name='relations')\n",
    "        self.tails = tf.placeholder(tf.int32, [self.batch_size], name='labels')\n",
    "        self.is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "\n",
    "        self.labels = tf.expand_dims(self.relations, -1)\n",
    "\n",
    "    def _build_embedding(self):\n",
    "        \n",
    "        self.entities_emb = tf.get_variable(\n",
    "            'entities', [self.n_entities, self.output_dim], tf.float32,\n",
    "            tf.contrib.layers.xavier_initializer())\n",
    "        self.relations_emb = tf.get_variable(\n",
    "            'relations', [self.n_relations, self.output_dim], tf.float32,\n",
    "            tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    def _build_aggre(self):\n",
    "        \n",
    "        self.hn_e = self.e_sampled_contexts[:, :, 0]\n",
    "        self.hn_r = self.e_sampled_contexts[:, :, 1]\n",
    "        \n",
    "        self.e_embs = []\n",
    "        self.e_embs.append(self.entities_emb)\n",
    "        \n",
    "        self.rn_h = self.r_sampled_contexts[:, :, 0]\n",
    "        self.rn_t = self.r_sampled_contexts[:, :, 1]\n",
    "        \n",
    "        self.r_embs = []\n",
    "        self.r_embs.append(self.relations_emb)\n",
    "        \n",
    "        #self.output_keep_prob = 1\n",
    "        for i in range(self.hops):\n",
    "                        \n",
    "            self.en_h_emb = tf.nn.embedding_lookup(self.e_embs[-1], self.hn_e)  # n_entity, 4, 64\n",
    "            self.en_r_emb = tf.nn.embedding_lookup(self.r_embs[-1], self.hn_r)  # n_entity, 4, 64  \n",
    "            self.e_context_info = self.e_context_aggregation(self.en_h_emb, self.en_r_emb, self.masks, i)\n",
    "            #self.e_context_info = tf.layers.dropout(self.e_context_info, 1-self.output_keep_prob, training = self.is_training)\n",
    "            self.new_e_emb = self.e_update(self.e_embs[-1], self.e_context_info, i)\n",
    "            \n",
    "            \n",
    "            self.rn_h_emb = tf.nn.embedding_lookup(self.e_embs[-1], self.rn_h)\n",
    "            self.rn_t_emb = tf.nn.embedding_lookup(self.e_embs[-1], self.rn_t)\n",
    "            self.r_context_info = self.r_context_aggregation(self.rn_h_emb, self.rn_t_emb, self.r_masks, i)\n",
    "            #self.r_context_info = tf.layers.dropout(self.r_context_info, 1-self.output_keep_prob, training = self.is_training)\n",
    "            self.new_r_emb = self.r_update(self.r_embs[-1], self.r_context_info, i)\n",
    "            \n",
    "            self.r_embs.append(self.new_r_emb)\n",
    "            self.e_embs.append(self.new_e_emb)\n",
    "                \n",
    "        self.h_emb = tf.nn.embedding_lookup(self.e_embs[-1], self.heads)\n",
    "        self.r_emb = tf.nn.embedding_lookup(self.r_embs[-1], self.relations)\n",
    "        self.t_emb = tf.nn.embedding_lookup(self.e_embs[-1], self.tails)\n",
    "\n",
    "        self.q_emb = self.h_emb * self.t_emb\n",
    "        self.b = tf.get_variable('bias', [self.n_relations], tf.float32,tf.contrib.layers.xavier_initializer())\n",
    "        self.scores = tf.matmul(self.q_emb, tf.transpose(self.r_embs[-1])) + self.b\n",
    "\n",
    "\n",
    "    def _build_train(self):\n",
    "        if self.negtive_num != -1:\n",
    "            self.base_loss = tf.reduce_mean(\n",
    "                tf.nn.sampled_softmax_loss(weights=self.r_embs[-1],\n",
    "                                           biases=self.b,\n",
    "                                           labels=self.labels,\n",
    "                                           inputs=self.q_emb,\n",
    "                                           num_sampled=self.negtive_num,\n",
    "                                           num_classes=self.n_relations))\n",
    "        else:\n",
    "            self.base_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=self.relations, logits=self.scores))\n",
    "\n",
    "        self.l2_loss = self.l2 * sum(tf.nn.l2_loss(var)\n",
    "            for var in tf.trainable_variables() if 'bias' not in var.name)\n",
    "        self.loss = self.base_loss + self.l2_loss\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "\n",
    "    def _build_eval(self):\n",
    "        self.scores_normalized = tf.nn.sigmoid(self.scores)\n",
    "        correct_predictions = tf.equal(self.relations, tf.cast(tf.argmax(self.scores, axis=-1), tf.int32))\n",
    "        self.acc = tf.reduce_mean(tf.cast(correct_predictions, tf.float64))\n",
    "\n",
    "    def train(self, sess, feed_dict):\n",
    "        return sess.run([self.optimizer, self.loss, self.acc], feed_dict)\n",
    "\n",
    "    def _eval(self, sess, feed_dict):\n",
    "        return sess.run([self.acc, self.scores_normalized], feed_dict)\n",
    "    \n",
    "    def r_context_aggregation(self, rn_t_emb, rn_h_emb, r_masks, hopi):\n",
    "        context_info = rn_h_emb * rn_t_emb\n",
    "        weight = tf.nn.softmax(tf.reduce_sum(context_info * tf.expand_dims(self.r_embs[-1],1), -1, True) * r_masks, 1)\n",
    "        return tf.reduce_sum(context_info * weight, 1)\n",
    "\n",
    "    def r_update(self, r_embs, r_context_info, hopi):\n",
    "        return r_embs + r_context_info \n",
    "    \n",
    "    def e_context_aggregation(self, en_h_emb, hn_r_emb, e_masks, hopi):\n",
    "        context_info = en_h_emb * hn_r_emb\n",
    "        weight = tf.nn.softmax(tf.reduce_sum(context_info * tf.expand_dims(self.e_embs[-1],1), -1, True) * e_masks, 1)\n",
    "        return tf.reduce_sum(context_info * weight, 1)\n",
    "    \n",
    "    def e_update(self, e_embs, e_context_info, hopi):\n",
    "        return e_embs + e_context_info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T12:35:11.651750Z",
     "start_time": "2020-09-20T12:35:11.623068Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "\n",
    "def dump_data(obj, wfpath, wfname):\n",
    "    with open(os.path.join(wfpath, wfname), 'wb') as wf:\n",
    "        pickle.dump(obj, wf)\n",
    "\n",
    "\n",
    "def load_file(rfpath, rfname):\n",
    "    with open(os.path.join(rfpath, rfname), 'rb') as rf:\n",
    "        return pickle.load(rf)\n",
    "\n",
    "\n",
    "def dim_factorization(d):\n",
    "    half = int(math.sqrt(d)) + 1\n",
    "    while d % half > 0:\n",
    "        half -= 1\n",
    "    x = half\n",
    "    y = d // half\n",
    "    assert x * y == d\n",
    "    print(\"dim factorization\", x, y)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def read_entities(file_name):\n",
    "    d = {}\n",
    "    file = open(file_name)\n",
    "    for line in file:\n",
    "        index, name = line.strip().split('\\t')\n",
    "        d[name] = int(index)\n",
    "    file.close()\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "def read_relations(file_name):\n",
    "    d = {}\n",
    "    file = open(file_name)\n",
    "    for line in file:\n",
    "        index, name = line.strip().split('\\t')\n",
    "        d[name] = int(index)\n",
    "    file.close()\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "def read_triplets(file_name):\n",
    "    data = []\n",
    "\n",
    "    file = open(file_name)\n",
    "    for line in file:\n",
    "        head, relation, tail = line.strip().split('\\t')\n",
    "\n",
    "        head_idx = entity_dict[head]\n",
    "        relation_idx = relation_dict[relation]\n",
    "        tail_idx = entity_dict[tail]\n",
    "\n",
    "        data.append((head_idx, relation_idx, tail_idx))\n",
    "        data.append((tail_idx, relation_idx, head_idx))\n",
    "\n",
    "    file.close()\n",
    "\n",
    "    return data\n",
    "\n",
    "    \n",
    "def load_data(dataset, context_samples_num):\n",
    "    global entity_dict, relation_dict\n",
    "    global n_relation, n_entity\n",
    "    directory = 'data/' + dataset + '/'\n",
    "\n",
    "    print('reading entity dict and relation dict ...')\n",
    "    entity_dict = read_entities(directory + 'entities.dict')\n",
    "    relation_dict = read_relations(directory + 'relations.dict')\n",
    "    \n",
    "    n_entity = len(entity_dict)\n",
    "    n_relation = len(relation_dict)\n",
    "        \n",
    "    print('entitiy  num:', n_entity)\n",
    "    print('relation num:', n_relation)\n",
    "\n",
    "    print('reading train, validation, and test data ...')\n",
    "    train_triplets = read_triplets(directory + 'train.txt')\n",
    "    valid_triplets = read_triplets(directory + 'valid.txt')\n",
    "    test_triplets = read_triplets(directory + 'test.txt')\n",
    "\n",
    "    triplets = [train_triplets, valid_triplets, test_triplets]\n",
    "\n",
    "    print('sampling contexts ...')\n",
    "    e_contexts = dict()\n",
    "    r_contexts = dict()\n",
    "    for (head_idx, relation_idx, tail_idx) in train_triplets:\n",
    "        if tail_idx not in e_contexts:\n",
    "            e_contexts[tail_idx] = []\n",
    "        e_contexts[tail_idx].append([head_idx, relation_idx])\n",
    "        \n",
    "        if relation_idx not in r_contexts:\n",
    "            r_contexts[relation_idx] = []\n",
    "        r_contexts[relation_idx].append([head_idx, tail_idx])\n",
    "    \n",
    "    e_sampled_contexts = []\n",
    "    e_masks = []\n",
    "    for h in range(n_entity):\n",
    "        mask = []\n",
    "        if h in e_contexts:\n",
    "            context_list = e_contexts[h]\n",
    "            if len(context_list) >= context_samples_num[0]:\n",
    "                idxs = np.random.choice(len(context_list), size=context_samples_num[0], replace=False) \n",
    "                e_ns = np.array(context_list)[idxs]\n",
    "                for n in e_ns:\n",
    "                    mask.append([1])\n",
    "            else:\n",
    "                lenth = context_samples_num[0] - len(context_list)\n",
    "                e_ns = np.pad(context_list, [[0,lenth],[0,0]])\n",
    "                for ii,n in enumerate(e_ns):\n",
    "                    if ii <len(context_list):\n",
    "                        mask.append([1])\n",
    "                    else:\n",
    "                        mask.append([0])\n",
    "        else:\n",
    "            e_ns = np.zeros((context_samples_num[0],2),dtype=np.int32)\n",
    "            mask = [[0]]*context_samples_num[0]\n",
    "            \n",
    "        e_sampled_contexts.append(e_ns)\n",
    "        e_masks.append(mask)\n",
    "    \n",
    "    r_sampled_contexts = []\n",
    "    r_masks = []\n",
    "    for r in range(n_relation):\n",
    "        mask = []\n",
    "        if r in r_contexts:\n",
    "            context_list = r_contexts[r]\n",
    "            if len(context_list) >= context_samples_num[1]:\n",
    "                idxs = np.random.choice(len(context_list), size=context_samples_num[1], replace=False) \n",
    "                r_ns = np.array(context_list)[idxs]\n",
    "                for n in r_ns:\n",
    "                    mask.append([1])\n",
    "            else:\n",
    "                lenth = context_samples_num[1] - len(context_list)\n",
    "                r_ns = np.pad(context_list, [[0,lenth],[0,0]])\n",
    "                for ii,n in enumerate(r_ns):\n",
    "                    if ii <len(context_list):\n",
    "                        mask.append([1])\n",
    "                    else:\n",
    "                        mask.append([0])\n",
    "        else:\n",
    "            r_ns = np.zeros((context_samples_num[1],2),dtype=np.int32)\n",
    "            mask = [[0]]*context_samples_num[1]\n",
    "            \n",
    "        r_sampled_contexts.append(r_ns)\n",
    "        r_masks.append(mask)\n",
    "\n",
    "    return triplets, (np.array(e_sampled_contexts,dtype=np.int32), np.array(r_sampled_contexts,dtype=np.int32)), (e_masks, r_masks)\n",
    "\n",
    "def evaluate(entity_pairs, labels, return_score=False):\n",
    "    acc_list = []\n",
    "    if return_score:\n",
    "        scores_list = []\n",
    "        \n",
    "    s = 0\n",
    "    while s + model.batch_size <= len(labels):\n",
    "        acc, scores = model._eval(\n",
    "            sess,\n",
    "            get_feed_dict(entity_pairs,\n",
    "                          labels,\n",
    "                          s,\n",
    "                          s + model.batch_size,\n",
    "                          training=False))\n",
    "        acc_list.append(acc)\n",
    "        if return_score:\n",
    "            scores_list.extend(scores)\n",
    "        s += model.batch_size\n",
    "\n",
    "    if return_score:\n",
    "        return float(np.mean(acc_list)), np.array(scores_list, np.float32)\n",
    "    else:\n",
    "        return float(np.mean(acc_list))\n",
    "\n",
    "\n",
    "def calculate_ranking_metrics(triplets, scores, true_relations):\n",
    "    for i in range(scores.shape[0]):\n",
    "        head, relation, tail = triplets[i]\n",
    "        for j in true_relations[head, tail] - {relation}:\n",
    "            scores[i, j] -= 1.0\n",
    "\n",
    "    sorted_indices = np.argsort(-scores, axis=1)\n",
    "    relations = np.array(triplets)[0:scores.shape[0], 1]\n",
    "    sorted_indices -= np.expand_dims(relations, 1)\n",
    "    zero_coordinates = np.argwhere(sorted_indices == 0)\n",
    "    rankings = zero_coordinates[:, 1] + 1\n",
    "\n",
    "    mrr = float(np.mean(1 / rankings))\n",
    "    mr = float(np.mean(rankings))\n",
    "    hit1 = float(np.mean(rankings <= 1))\n",
    "    hit3 = float(np.mean(rankings <= 3))\n",
    "    hit5 = float(np.mean(rankings <= 5))\n",
    "    hit10 = float(np.mean(rankings <= 10))\n",
    "\n",
    "    return mrr, mr, hit1, hit3, hit5, hit10\n",
    "\n",
    "\n",
    "def get_feed_dict(entity_pairs, labels, start, end, training):\n",
    "    feed_dict = {}\n",
    "    feed_dict[model.heads] = entity_pairs[start:end, 0]\n",
    "    feed_dict[model.tails] = entity_pairs[start:end, 1]\n",
    "    feed_dict[model.relations] = labels[start:end]\n",
    "    feed_dict[model.is_training] = training\n",
    "\n",
    "    return feed_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T12:37:18.028747Z",
     "start_time": "2020-09-20T12:35:11.653339Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading entity dict and relation dict ...\n",
      "entitiy  num: 14541\n",
      "relation num: 237\n",
      "reading train, validation, and test data ...\n",
      "sampling contexts ...\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "start training ...\n",
      "epoch  0   train acc: 0.7816   valid acc: 0.9189   test acc: 0.9181\n",
      "           mrr: 0.9566   mr: 1.2269   h1: 0.9250   h3: 0.9881   h5: 0.9949   h10: 0.9975\n",
      "\n",
      "epoch  1   train acc: 0.8092   valid acc: 0.9160   test acc: 0.9161\n",
      "           mrr: 0.9607   mr: 1.1785   h1: 0.9328   h3: 0.9884   h5: 0.9950   h10: 0.9982\n",
      "\n",
      "epoch  2   train acc: 0.8163   valid acc: 0.9184   test acc: 0.9191\n",
      "           mrr: 0.9630   mr: 1.1833   h1: 0.9376   h3: 0.9877   h5: 0.9940   h10: 0.9975\n",
      "\n",
      "epoch  3   train acc: 0.8226   valid acc: 0.9138   test acc: 0.9129\n",
      "           mrr: 0.9591   mr: 1.2352   h1: 0.9307   h3: 0.9857   h5: 0.9926   h10: 0.9964\n",
      "\n",
      "epoch  4   train acc: 0.8243   valid acc: 0.9166   test acc: 0.9162\n",
      "           mrr: 0.9619   mr: 1.1989   h1: 0.9354   h3: 0.9879   h5: 0.9949   h10: 0.9976\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f1b02265cc60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m                               \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                               \u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                               training=True))\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-abcaef3480fd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sess, feed_dict)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf1/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf1/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf1/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf1/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf1/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf1/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "global model, sess\n",
    "\n",
    "triplets, sampled_contexts, masks = load_data('wn18rr', context_samples_num = [4,32]) \n",
    "#triplets, sampled_contexts, masks = load_data('FB15k-237', context_samples_num = [8,4])\n",
    "\n",
    "train_triplets, valid_triplets, test_triplets = triplets\n",
    "\n",
    "train_entity_pairs = np.array([[triplet[0], triplet[2]]\n",
    "                               for triplet in train_triplets], np.int32)\n",
    "valid_entity_pairs = np.array([[triplet[0], triplet[2]]\n",
    "                               for triplet in valid_triplets], np.int32)\n",
    "test_entity_pairs = np.array([[triplet[0], triplet[2]]\n",
    "                              for triplet in test_triplets], np.int32)\n",
    "\n",
    "train_labels = np.array([triplet[1] for triplet in train_triplets], np.int32)\n",
    "valid_labels = np.array([triplet[1] for triplet in valid_triplets], np.int32)\n",
    "test_labels  = np.array([triplet[1] for triplet in test_triplets], np.int32)\n",
    "\n",
    "# prepare for top-k evaluation\n",
    "true_relations = defaultdict(set)\n",
    "for head, relation, tail in train_triplets + valid_triplets + test_triplets:\n",
    "    true_relations[(head, tail)].add(relation)\n",
    "\n",
    "model = AggrE(epoch=20,\n",
    "             batch_size=512,\n",
    "             dim=256,\n",
    "             l2=1e-7,\n",
    "             lr=5e-3,\n",
    "             negtive_num=-1,\n",
    "             sampled_contexts=sampled_contexts,\n",
    "             masks = masks,\n",
    "             hops=2)\n",
    "'''\n",
    "model = AggrE(epoch=20,\n",
    "             batch_size=1024,\n",
    "             dim=256,\n",
    "             l2=1e-6,\n",
    "             lr=5e-3,\n",
    "             negtive_num=-1,\n",
    "             sampled_contexts=sampled_contexts,\n",
    "             masks = masks,\n",
    "             hops=4) #FB15k-237\n",
    "'''\n",
    "\n",
    "best_valid_acc = 0.0\n",
    "final_res = None  # acc, mrr, mr, hit1, hit3, hit5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print('start training ...')\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(model.epoch):\n",
    "\n",
    "        # shuffle training data\n",
    "        index = np.arange(len(train_labels))\n",
    "        np.random.shuffle(index)\n",
    "        train_entity_pairs = train_entity_pairs[index]\n",
    "        train_labels = train_labels[index]\n",
    "\n",
    "        # training\n",
    "        s = 0\n",
    "        while s + model.batch_size <= len(train_labels):\n",
    "            _, loss, acc = model.train(\n",
    "                sess,\n",
    "                get_feed_dict(train_entity_pairs,\n",
    "                              train_labels,\n",
    "                              s,\n",
    "                              s + model.batch_size,\n",
    "                              training=True))\n",
    "            s += model.batch_size\n",
    "\n",
    "            if s % (model.batch_size*300) == 0:\n",
    "                # evaluation\n",
    "                print('epoch %2d   ' % step, end='')\n",
    "                train_acc = evaluate(train_entity_pairs, train_labels)\n",
    "                valid_acc = evaluate(valid_entity_pairs, valid_labels)\n",
    "                test_acc, test_scores = evaluate(test_entity_pairs, test_labels, return_score=True)\n",
    "\n",
    "                # show evaluation result for current epoch\n",
    "                current_res = 'acc: %.4f' % test_acc\n",
    "                print('train acc: %.4f   valid acc: %.4f   test acc: %.4f' % (train_acc, valid_acc, test_acc))\n",
    "\n",
    "                mrr, mr, hit1, hit3, hit5, hit10 = calculate_ranking_metrics(\n",
    "                    test_triplets, test_scores, true_relations)\n",
    "                current_res += '   mrr: %.4f   mr: %.4f   h1: %.4f   h3: %.4f   h5: %.4f   h10: %.4f' % (\n",
    "                    mrr, mr, hit1, hit3, hit5, hit10)\n",
    "                print('           mrr: %.4f   mr: %.4f   h1: %.4f   h3: %.4f   h5: %.4f   h10: %.4f'\n",
    "                    % (mrr, mr, hit1, hit3, hit5, hit10))\n",
    "                print()\n",
    "                              \n",
    "                # update final results according 2to validation accuracy\n",
    "                if valid_acc > best_valid_acc:\n",
    "                    best_valid_acc = valid_acc\n",
    "                    final_res = current_res\n",
    "\n",
    "    # show final evaluation result\n",
    "    print('final results\\n%s' % final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.15",
   "language": "python",
   "name": "tf1.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 328.333666,
   "position": {
    "height": "349.667px",
    "left": "417px",
    "right": "20px",
    "top": "40px",
    "width": "782.323px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
